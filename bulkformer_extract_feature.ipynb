{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc45937",
   "metadata": {},
   "source": [
    "### BulkFormer feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a16039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2370d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
   
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,DataLoader,random_split\n",
    "from torch_geometric.typing import SparseTensor\n",
    "from utils.BulkFormer import BulkFormer\n",
    "from model.config import model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2afcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = 'cuda'\n",
    "graph_path = 'data/G_tcga.pt'\n",
    "weights_path = 'data/G_tcga_weight.pt'\n",
    "gene_emb_path = 'data/esm2_feature_concat.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5485ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BulkFormer model with preloaded graph structure and gene embeddings.\n",
    "graph = torch.load(graph_path, map_location='cpu', weights_only=False)\n",
    "weights = torch.load(weights_path, map_location='cpu', weights_only=False)\n",
    "graph = SparseTensor(row=graph[1], col=graph[0], value=weights).t().to(device)\n",
    "gene_emb = torch.load(gene_emb_path, map_location='cpu', weights_only=False)\n",
    "model_params['graph'] = graph\n",
    "model_params['gene_emb'] = gene_emb\n",
    "model = BulkFormer(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bec646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained BulkFormer model checkpoint for inference or fine-tuning.\n",
    "ckpt_model = torch.load('model/bulkformer_147M.pt',weights_only=False)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in ckpt_model.items():\n",
    "    new_key = key[7:] if key.startswith(\"module.\") else key\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f97af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_df, gene_length_dict):\n",
    "    \"\"\"\n",
    "    Normalize RNA-seq count data to log-transformed TPM values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_df : pandas.DataFrame\n",
    "        A gene expression matrix where rows represent samples and columns represent genes.\n",
    "        Each entry contains the raw read count of a gene in a given sample.\n",
    "\n",
    "    gene_length_dict : dict\n",
    "        A dictionary mapping gene identifiers (Ensembl gene IDs) to gene lengths (in base pairs).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_tpm_df : pandas.DataFrame\n",
    "        A DataFrame of the same shape as `X_df`, containing log-transformed TPM values\n",
    "        (i.e., log(TPM + 1)) for each gene in each sample.\n",
    "\n",
    "    Description\n",
    "    -----------\n",
    "    This function converts raw RNA-seq count data to transcripts per million (TPM) values by\n",
    "    normalizing for gene length and sample-specific total expression. Gene lengths are provided\n",
    "    via `gene_length_dict`, and genes not present in the dictionary are assigned a default\n",
    "    length of 1,000 bp (equivalent to no correction). The resulting TPM values are subsequently\n",
    "    log-transformed using the natural logarithm (log1p). This normalization procedure accounts\n",
    "    for both gene length and sequencing depth, facilitating cross-sample and cross-gene comparisons.\n",
    "    \"\"\"\n",
    "    gene_names = X_df.columns\n",
    "    gene_lengths_kb = np.array([gene_length_dict.get(gene, 1000) / 1000  for gene in gene_names])\n",
    "    counts_matirx = X_df.values\n",
    "    rate = counts_matirx / gene_lengths_kb\n",
    "    sum_per_sample = rate.sum(axis=1)\n",
    "    sum_per_sample[sum_per_sample == 0] = 1e-6  \n",
    "    sum_per_sample = sum_per_sample.reshape(-1, 1)\n",
    "    tpm = rate / sum_per_sample * 1e6\n",
    "    log_tpm = np.log1p(tpm)\n",
    "    log_tpm_df = pd.DataFrame(log_tpm,index=X_df.index, columns=X_df.columns)\n",
    "    return log_tpm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f89a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_gene_selection(X_df, gene_list):\n",
    "    \"\"\"\n",
    "    Aligns a gene expression matrix to a predefined gene list by adding placeholder values\n",
    "    for missing genes and generating a binary mask indicating imputed entries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_df : pandas.DataFrame\n",
    "        A gene expression matrix with rows representing samples and columns representing genes.\n",
    "        The entries are typically log-transformed or normalized expression values.\n",
    "\n",
    "    gene_list : list of str\n",
    "        A predefined list of gene identifiers (Ensembl Gene IDs) to be retained\n",
    "        in the final matrix. This list defines the desired gene space for downstream analyses.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_df : pandas.DataFrame\n",
    "        A gene expression matrix aligned to `gene_list`, with missing genes filled with a constant\n",
    "        placeholder value (−10) and columns ordered accordingly.\n",
    "\n",
    "    to_fill_columns : list of str\n",
    "        A list of genes from `gene_list` that were not present in the original `X_df`\n",
    "        and were therefore added with placeholder values.\n",
    "\n",
    "    var : pandas.DataFrame\n",
    "        A DataFrame with one row per gene, containing a binary column `'mask'` indicating\n",
    "        whether a gene was imputed (1) or originally present (0). This can be used for masking\n",
    "        in training or evaluation of models that distinguish observed and imputed entries.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function ensures that all samples share a consistent gene space, which is essential\n",
    "    for tasks such as model training, cross-dataset integration, or visualization. Placeholder\n",
    "    values (−10) are used to maintain matrix shape while avoiding unintended bias in downstream\n",
    "    statistical analyses or machine learning models.\n",
    "    \"\"\"\n",
    "    to_fill_columns = list(set(gene_list) - set(X_df.columns))\n",
    "\n",
    "    padding_df = pd.DataFrame(np.full((X_df.shape[0], len(to_fill_columns)), -10), \n",
    "                            columns=to_fill_columns, \n",
    "                            index=X_df.index)\n",
    "\n",
    "    X_df = pd.DataFrame(np.concatenate([df.values for df in [X_df, padding_df]], axis=1), \n",
    "                        index=X_df.index, \n",
    "                        columns=list(X_df.columns) + list(padding_df.columns))\n",
    "    X_df = X_df[gene_list]\n",
    "    \n",
    "    var = pd.DataFrame(index=X_df.columns)\n",
    "    var['mask'] = [1 if i in to_fill_columns else 0 for i in list(var.index)]\n",
    "    return X_df, to_fill_columns,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d80458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(expr_array, \n",
    "                    output_feature_type,\n",
    "                    aggregate_type,\n",
    "                    device,\n",
    "                    batch_size,\n",
    "                    mask_prob = 0.1,\n",
    "                    interested_gene_idx = None,\n",
    "                    output_expr = False,\n",
    "                    esm2_emb = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts sample-level or gene-level feature representations from input expression profiles\n",
    "    using a pre-trained deep learning model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    expr_array : np.ndarray\n",
    "        A NumPy array of shape [N_samples, N_genes] representing gene expression profiles\n",
    "        (e.g., log-transformed TPM values).\n",
    "\n",
    "    mask_prob : float, optional\n",
    "        Masking probability used during feature extraction to simulate missing genes.\n",
    "        This parameter controls the fraction of randomly masked genes during inference,\n",
    "        enabling robustness to incomplete gene sets relative to the pretrained vocabulary.\n",
    "\n",
    "    interested_gene_idx : list or np.ndarray\n",
    "        Indices of your interested genes used for sample-level embedding aggregation.\n",
    "\n",
    "    feature_type : str\n",
    "        Specifies the type of feature to extract. Options:\n",
    "            - 'sample_level': aggregate gene embeddings to a single sample-level vector.\n",
    "            - 'gene_level': retain per-gene embeddings for downstream fusion with external embeddings (ESM2).\n",
    "\n",
    "    aggregate_type : str\n",
    "        Aggregation method used when `feature_type='sample_level'`. Options include:\n",
    "            - 'max': use maximum value across selected genes.\n",
    "            - 'mean': use average value.\n",
    "            - 'median': use median value.\n",
    "            - 'all': combine all three strategies by summation.\n",
    "\n",
    "    device : torch.device\n",
    "        Computation device (e.g., 'cuda' or 'cpu') for model inference.\n",
    "\n",
    "    batch_size : int\n",
    "        Number of samples per batch during feature extraction.\n",
    "\n",
    "    output_expr : bool, optional\n",
    "        If True, return predicted gene expression values instead of extracted embeddings.\n",
    "\n",
    "    esm2_emb : torch.Tensor, optional\n",
    "        Precomputed ESM2 embeddings for all genes, used in gene-level feature concatenation.\n",
    "        Required if `feature_type='gene_level'`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_emb : torch.Tensor\n",
    "        The extracted feature representations:\n",
    "            - [N_samples, D] for sample-level features.\n",
    "            - [N_samples, N_genes, D_concat] for gene-level features.\n",
    "\n",
    "    or (if `output_expr=True`)\n",
    "    expr_predictions : np.ndarray\n",
    "        Model-predicted expression profiles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert NumPy array to PyTorch tensor and move to target device\n",
    "    expr_tensor = torch.tensor(expr_array, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Create dataset and dataloader for batched inference\n",
    "    mydataset = TensorDataset(expr_tensor)\n",
    "    myloader = DataLoader(mydataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    all_emb_list = []\n",
    "    all_pred_expr_list = []\n",
    "\n",
    "    # Use inference mode with automatic mixed precision on GPU\n",
    "    with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=True):\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 1. SAMPLE-LEVEL FEATURE EXTRACTION\n",
    "        # ----------------------------------------------------------\n",
    "        if output_feature_type == 'sample_level':\n",
    "            for (X,) in tqdm(myloader, total=len(myloader)):\n",
    "                X = X.to(device)\n",
    "\n",
    "                # Obtain per-gene embeddings from the model\n",
    "                gene_emb = model(X, mask_prob=mask_prob, output_expr=output_expr)\n",
    "                gene_emb = gene_emb.detach().cpu().numpy()\n",
    "\n",
    "                # Select interested genes if user provided gene indices\n",
    "                if interested_gene_idx:\n",
    "                    gene_emb = gene_emb[:, interested_gene_idx, :]\n",
    "\n",
    "                # Perform aggregation to obtain sample-level embeddings\n",
    "                if aggregate_type == 'max':\n",
    "                    final_emb = np.max(gene_emb, axis=1)\n",
    "                elif aggregate_type == 'mean':\n",
    "                    final_emb = np.mean(gene_emb, axis=1)\n",
    "                elif aggregate_type == 'median':\n",
    "                    final_emb = np.median(gene_emb, axis=1)\n",
    "                elif aggregate_type == 'all':\n",
    "                    max_emb = np.max(gene_emb, axis=1)\n",
    "                    mean_emb = np.mean(gene_emb, axis=1)\n",
    "                    median_emb = np.median(gene_emb, axis=1)\n",
    "                    final_emb = max_emb + mean_emb + median_emb\n",
    "\n",
    "                all_emb_list.append(final_emb)\n",
    "\n",
    "                # Memory cleanup for large batch inference\n",
    "                del gene_emb\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Stack all embeddings across batches\n",
    "            result_emb = np.vstack(all_emb_list)\n",
    "            result_emb = torch.tensor(result_emb, device='cpu', dtype=torch.float32)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 2. GENE-LEVEL FEATURE EXTRACTION (OPTIONALLY CONCAT WITH ESM2)\n",
    "        # ----------------------------------------------------------\n",
    "        elif output_feature_type == 'gene_level':\n",
    "            for (X,) in tqdm(myloader, total=len(myloader)):\n",
    "                X = X.to(device)\n",
    "\n",
    "                gene_emb = model(X, mask_prob=mask_prob, output_expr=output_expr)\n",
    "                gene_emb = gene_emb.detach().cpu().numpy()\n",
    "                all_emb_list.append(gene_emb)\n",
    "\n",
    "                del gene_emb\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Stack to [N_samples, N_genes, D]\n",
    "            all_emb = np.vstack(all_emb_list)\n",
    "            all_emb_tensor = torch.tensor(all_emb, device='cpu', dtype=torch.float32)\n",
    "\n",
    "            # Expand ESM2 embeddings to align with batch dimension: [N_samples, N_genes, D_esm2]\n",
    "            esm2_emb_expanded = esm2_emb.unsqueeze(0).expand(all_emb_tensor.shape[0], -1, -1)\n",
    "\n",
    "            # Concatenate model embeddings with ESM2 embeddings\n",
    "            result_emb = torch.cat([all_emb_tensor, esm2_emb_expanded], dim=-1)\n",
    "\n",
    "        # ----------------------------------------------------------\n",
    "        # 3. EXPRESSION-LEVEL OUTPUT (MODEL PREDICTS EXPRESSION)\n",
    "        # ----------------------------------------------------------\n",
    "        elif output_feature_type == 'expr_level':\n",
    "            output_expr = True\n",
    "            for (X,) in tqdm(myloader, total=len(myloader)):\n",
    "                X = X.to(device)\n",
    "\n",
    "                pred_expr = model(X, mask_prob=mask_prob, output_expr=output_expr)\n",
    "                all_pred_expr_list.append(pred_expr.detach().cpu().numpy())\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Final return logic\n",
    "    # ----------------------------------------------------------\n",
    "    if output_expr:\n",
    "        return np.vstack(all_pred_expr_list)\n",
    "    else:\n",
    "        return result_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3718e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo normalized data (log-transformed TPM)\n",
    "log_tpm_df = pd.read_csv('data/demo_normalized_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18ede26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo count data (raw count)\n",
    "count_df = pd.read_csv('data/demo_count_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d6a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw counts to normalized expression values (log-transformed TPM)\n",
    "gene_length_df = pd.read_csv('data/gene_length_df.csv')\n",
    "gene_length_dict = gene_length_df.set_index('ensg_id')['length'].to_dict()\n",
    "log_tpm_df = normalize_data(X_df=count_df, gene_length_dict=gene_length_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55cbafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bulkformer_gene_info = pd.read_csv('data/bulkformer_gene_info.csv')\n",
    "bulkformer_gene_list = bulkformer_gene_info['ensg_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05327887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align expression data to a predefined gene list with placeholder imputation for missing genes.\n",
    "input_df , to_fill_columns, var= main_gene_selection(X_df=log_tpm_df,gene_list=bulkformer_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8445438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask prob: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute the gene-missing rate (masking probability) for the input data.\n",
    "# This measures the proportion of genes in the sample that are absent from the pretrained vocabulary of 200,10 genes. \n",
    "mask_prob = len(list(var[var['mask'] == 1].index)) / var.shape[0]\n",
    "print(f\"mask prob: {mask_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e934a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of genes of interest (each entry must be an Ensembl gene ID\n",
    "# and must exist in the BulkFormer vocabulary). These genes will be used to\n",
    "# aggregate their corresponding embeddings into a sample-level representation.\n",
    "interested_gene_idx = torch.load('data/interested_gene_list.pt',weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1ec8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract sample-level embedding\n",
    "res1 = extract_feature(\n",
    "    expr_array= input_df.values[:16],\n",
    "    interested_gene_idx=interested_gene_idx,\n",
    "    output_feature_type='sample_level',\n",
    "    aggregate_type='mean',\n",
    "    device=device,\n",
    "    batch_size=4,\n",
    "    output_expr=False,\n",
    "    mask_prob=mask_prob,\n",
    "    esm2_emb=model_params['gene_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a82275bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 643])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36e89a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 643])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b38e50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract gene-level embedding\n",
    "res2 = extract_feature(\n",
    "    expr_array= input_df.values[:16],\n",
    "    interested_gene_idx=interested_gene_idx,\n",
    "    output_feature_type='gene_level',\n",
    "    aggregate_type='mean', \n",
    "    device=device,\n",
    "    batch_size=4,\n",
    "    output_expr=False,\n",
    "    mask_prob=mask_prob,\n",
    "    esm2_emb=model_params['gene_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d1932cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 20010, 1923])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bfdb71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4820,  0.3869, -0.7701,  ..., -0.0972, -0.1156, -0.0694],\n",
       "         [-0.1802, -1.1161, -0.2645,  ..., -0.0929, -0.0102,  0.0749],\n",
       "         [ 0.1359,  0.2348, -0.6361,  ..., -0.1507, -0.0174,  0.1455],\n",
       "         ...,\n",
       "         [-1.1062, -0.3240, -0.8222,  ..., -0.0316,  0.0078,  0.0943],\n",
       "         [-0.7061, -1.3536, -0.5405,  ..., -0.0891, -0.0469,  0.1897],\n",
       "         [-0.5652, -0.8169, -0.2480,  ..., -0.0528, -0.0946,  0.0670]],\n",
       "\n",
       "        [[ 1.0777,  0.8992,  0.9000,  ..., -0.0972, -0.1156, -0.0694],\n",
       "         [ 0.0287, -1.3930, -0.3553,  ..., -0.0929, -0.0102,  0.0749],\n",
       "         [ 0.1787, -0.9308,  0.1065,  ..., -0.1507, -0.0174,  0.1455],\n",
       "         ...,\n",
       "         [-1.0591, -0.1040, -0.5476,  ..., -0.0316,  0.0078,  0.0943],\n",
       "         [-0.5118, -1.4339, -0.5836,  ..., -0.0891, -0.0469,  0.1897],\n",
       "         [-0.4605, -0.9980, -0.3781,  ..., -0.0528, -0.0946,  0.0670]],\n",
       "\n",
       "        [[ 1.2243,  0.1527,  0.8426,  ..., -0.0972, -0.1156, -0.0694],\n",
       "         [-0.2182, -1.4306, -0.5273,  ..., -0.0929, -0.0102,  0.0749],\n",
       "         [-0.3675,  0.0622,  0.6967,  ..., -0.1507, -0.0174,  0.1455],\n",
       "         ...,\n",
       "         [ 0.5191,  1.2344, -0.0449,  ..., -0.0316,  0.0078,  0.0943],\n",
       "         [-0.5602, -1.2075, -0.7620,  ..., -0.0891, -0.0469,  0.1897],\n",
       "         [-0.5753, -0.7380, -0.3267,  ..., -0.0528, -0.0946,  0.0670]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.1221, -0.0065, -0.4675,  ..., -0.0972, -0.1156, -0.0694],\n",
       "         [-0.2360, -1.4700, -0.5417,  ..., -0.0929, -0.0102,  0.0749],\n",
       "         [ 0.3485, -1.1898, -0.5623,  ..., -0.1507, -0.0174,  0.1455],\n",
       "         ...,\n",
       "         [ 0.4123, -0.0542, -0.4960,  ..., -0.0316,  0.0078,  0.0943],\n",
       "         [-0.7029, -1.4425, -0.5893,  ..., -0.0891, -0.0469,  0.1897],\n",
       "         [-0.4595, -0.9060, -0.1044,  ..., -0.0528, -0.0946,  0.0670]],\n",
       "\n",
       "        [[ 0.8015,  0.2446,  0.4571,  ..., -0.0972, -0.1156, -0.0694],\n",
       "         [-0.2877, -1.3953, -0.3162,  ..., -0.0929, -0.0102,  0.0749],\n",
       "         [-0.2519, -0.1840, -0.5299,  ..., -0.1507, -0.0174,  0.1455],\n",
       "         ...,\n",
       "         [-0.6371,  0.4634, -1.1864,  ..., -0.0316,  0.0078,  0.0943],\n",
       "         [-0.8009, -1.3100, -0.5125,  ..., -0.0891, -0.0469,  0.1897],\n",
       "         [-0.6152, -0.5331, -0.2668,  ..., -0.0528, -0.0946,  0.0670]],\n",
       "\n",
       "        [[ 1.2597,  1.2390,  0.1828,  ..., -0.0972, -0.1156, -0.0694],\n",
       "         [ 2.2696, -1.1568, -0.3468,  ..., -0.0929, -0.0102,  0.0749],\n",
       "         [-0.5082,  0.4266, -0.3614,  ..., -0.1507, -0.0174,  0.1455],\n",
       "         ...,\n",
       "         [-1.6035,  0.2116, -0.5411,  ..., -0.0316,  0.0078,  0.0943],\n",
       "         [-0.4563, -1.4194, -0.5162,  ..., -0.0891, -0.0469,  0.1897],\n",
       "         [-0.5480, -0.8246, -0.2344,  ..., -0.0528, -0.0946,  0.0670]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3369f68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract expression values\n",
    "res3 = extract_feature(\n",
    "    expr_array= input_df.values[:16],\n",
    "    interested_gene_idx=interested_gene_idx,\n",
    "    output_feature_type='expr_level',\n",
    "    aggregate_type='mean', \n",
    "    device=device,\n",
    "    batch_size=4,\n",
    "    output_expr=True,\n",
    "    mask_prob=mask_prob,\n",
    "    esm2_emb=model_params['gene_emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb76cce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 20010)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c579789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9785156 , 0.        , 3.1601562 , ..., 3.1289062 , 0.        ,\n",
       "        0.        ],\n",
       "       [3.2695312 , 0.        , 3.7070312 , ..., 3.953125  , 0.        ,\n",
       "        0.        ],\n",
       "       [2.5527344 , 0.        , 3.46875   , ..., 2.2949219 , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.91552734, 0.        , 3.0742188 , ..., 2.4179688 , 0.        ,\n",
       "        0.        ],\n",
       "       [2.0390625 , 0.        , 3.6367188 , ..., 4.3359375 , 0.        ,\n",
       "        0.        ],\n",
       "       [3.3496094 , 0.77246094, 3.3085938 , ..., 3.5820312 , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d5ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bulk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
